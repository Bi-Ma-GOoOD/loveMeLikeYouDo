{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skipgrams in Keras\n",
    "\n",
    "- In this lecture, we will implement Skipgrams in `Keras`.\n",
    "\n",
    "#### Loading in and preprocessing data\n",
    "- Load the Alice in Wonderland data in Corpus using Keras utility\n",
    "- `Keras` has some nice text preprocessing features too!\n",
    "- Split the text into sentences.\n",
    "- Use `Keras`' `Tokenizer` to tokenize sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# nltk\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# keras\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# keras\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation\n",
    "from keras.utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import model_to_dot \n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use Alice in Wonderland\n",
    "\n",
    "path = get_file('carrol-alice.txt', origin=\"http://www.gutenberg.org/files/11/11-0.txt\")\n",
    "corpus = open(path, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Split document into sentences first\n",
    "corpus = corpus[corpus.index('\\n\\n')+2:]  # remove header.\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "# Tokenize using Keras\n",
    "base_filter='!\"#$%&()*+,-./:;`<=>?@[\\\\]^_{|}~\\t\\n“”' + \"'\"\n",
    "tokenizer = Tokenizer(filters=base_filter)\n",
    "tokenizer.fit_on_texts(sentences)  # tokenizer assigns a unique integer index to each unique word in the text\n",
    "                                   # also characters in base_filter will be removed from the text\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "print(len(sequences), tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word Index:\", tokenizer.word_index)\n",
    "print(\"Word Counts:\", tokenizer.word_counts)\n",
    "print(\"Document Count:\", tokenizer.document_count)\n",
    "print(\"Word Document Frequency:\", tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what is happening;\n",
    "\n",
    "print(sentences[324])  # this is a sentence\n",
    "print(sentences[324].split())\n",
    "print(sequences[324])  # this is the same sentence where words are encoded as numbers.\n",
    "print(list(tokenizer.word_index[word.lower().replace(',', '').replace('.', '').replace('“', '').replace('”', '')] \n",
    "           for word in sentences[324].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skipgrams: Generating Input and Output Labels\n",
    "- Now that we have sentences, and word tokenization, we are in good position to create our training set for skipgrams.\n",
    "- Now we need to generate our `X_train` and `y_train`\n",
    "\n",
    "#### In the context of Word2Vec and skip-gram models:\n",
    "- *Target Word* is the word for which you are trying to predict the context words.\n",
    "- *Context words* are the words that appear in the vicinity (context) of the target word.\n",
    "\n",
    "- *couples* This is a list of tuples, where each tuple contains a pair of words (integer indices) representing a skip-gram pair. For example, (2, 45) might represent a target word at index 2 and a context word at index 45. The pairs are generated based on the specified window size around each target word.\n",
    "- *labels* This is a list of binary labels associated with each skip-gram pair. A label of 1 typically indicates that the context word is a true context word (a positive sample), and a label of 0 indicates a negative sample (a randomly selected word that is not the actual context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's first see how Keras' skipgrams function works.\n",
    "# The integer indices assigned to actual words start from 1 and padding token at index 0 allows us to represent sequences with variable lengths while still maintaining a consistent input size for the neural network.\n",
    "\n",
    "print(sentences[324], '\\n') \n",
    "couples, labels = skipgrams(sequences[324], len(tokenizer.word_index) + 1,\n",
    "    window_size=2, negative_samples=0, shuffle=True,\n",
    "    categorical=False, sampling_table=None)\n",
    "\n",
    "index_2_word = {val: key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "for (w1, w2), l in zip(couples, labels):\n",
    "    if w1 == tokenizer.word_index['temper']:\n",
    "        print(f'{index_2_word[w1]:{len(index_2_word[w1])}} - {index_2_word[w2]:10} {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Dimension to reduce to\n",
    "embedding_dim = 100\n",
    "window_size = 2\n",
    "\n",
    "def generate_data(sequences, window_size, vocab_size):\n",
    "    for seq in sequences:\n",
    "        X, y = [], []\n",
    "        couples, _ = skipgrams(\n",
    "            seq, vocab_size,\n",
    "            window_size=window_size, negative_samples=0, shuffle=True,\n",
    "            categorical=False, sampling_table=None)\n",
    "        if not couples:\n",
    "            continue\n",
    "        for target, context in couples:\n",
    "            X.append(target)\n",
    "            y.append(keras.utils.to_categorical(context, vocab_size))\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(len(X), 1)\n",
    "        y = y.reshape(len(X), vocab_size)\n",
    "        yield X, y\n",
    "        \n",
    "data_generator = generate_data(sequences, window_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Creating the Model\n",
    "* Lastly, we create the (shallow) network!\n",
    "* In Embedding layer in Keras, \n",
    "  * *input_dim* specifies the size of the vocabulary, i.e., the total number of distinct words in your dataset. It essentially sets the upper limit for the word indices that the layer can expect as input.\n",
    "  * *output_dim* specifies the size of the dense embedding vector for each input word.\n",
    "  * embeddings_initializer=*'glorot_uniform'* (also called Xavier initialization) initializes weights with a uniform distribution in a specific range, ensuring balanced gradient flow. This is useful for stabilizing training.\n",
    "  * *input_length* defines the length of the input sequence (i.e., how many tokens/words are expected in each input sample).\n",
    "Since this is a skip-gram model, each input is a single word, hence input_length=1.\n",
    "* During the training process of the Embedding layer, the updates to the embedding matrix are based on the specific word indices present in the training examples. It's not necessary to train all indices simultaneously.\n",
    "* To get the weights of the Embedding layer:\n",
    "  ```\n",
    "  embedding_weights = model.get_weights()[0]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                       embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((embedding_dim,)))\n",
    "skipgram.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "print('vocab_size:', vocab_size)\n",
    "print('embedding_dim:', embedding_dim)\n",
    "skipgram.summary()\n",
    "SVG(model_to_dot(skipgram, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Compiling and Training\n",
    "- Time to compile and train\n",
    "- We use crossentropy, common loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Keras Model\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the Skipgrams\n",
    "for iteration in range(1, 11):\n",
    "    loss, cnt = 0, 0\n",
    "    for x, y in generate_data(sequences, window_size, vocab_size):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "        cnt += 1\n",
    "    print('iteration {}, avg. loss is {}'.format(iteration, loss/cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Looking at the vectors\n",
    "\n",
    "To get word_vectors now, we look at the weights of the first layer.\n",
    "\n",
    "Let's also write functions giving us similarity of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = skipgram.get_weights()[0]\n",
    "\n",
    "def get_similarity(w1, w2):\n",
    "    i1, i2 = tokenizer.word_index[w1], tokenizer.word_index[w2]\n",
    "    v1, v2 = word_vectors[i1], word_vectors[i2]\n",
    "    return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 'queen'\n",
    "similar_words  = {word: get_similarity(w1, word) \n",
    "                  for word in tokenizer.word_index.keys() if word != w1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(similar_words.items(), key=lambda item: item[1])[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">(100 คะแนน) ปรับจูนโมเดลให้ดีขึ้นเพื่อหาคำที่ใกล้เคียงกับคำที่ระบุ</span>\n",
    "### * (50 คะแนน) ให้แสดงคำ 10 คำแรกที่ใกล้เคียงกับ queen มากที่สุดพร้อมค่าตัววัด\n",
    "### * (50 คะแนน) ให้แสดงคำ 10 คำแรกที่ใกล้เคียงกับ queen น้อยที่สุดพร้อมค่าตัววัด  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dl_env_1115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
